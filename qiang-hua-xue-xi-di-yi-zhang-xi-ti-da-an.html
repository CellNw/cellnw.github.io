<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>强化学习第一章习题答案</title><meta name="description" content="以下为个人答案、官方答案在文末 问： 假设上面的强化学习算法不是对战随机对手，而是以左右互博的方式自己与自己对战来训练自己。你认为在这种情况下会发生怎样的事情？它是否会学习到不同的策略？ 答： 1、我认为在训练初期，两者的价值函数$V(\cdot)$会震荡，这是因为在训练初期，模型并不是很会玩井字棋，所以两个模型一方面学会利用对方的破绽，另一方面也会弥补自己的破绽，所以价值函数$V(\cdot)$会随着这一过程变化。如果训练顺利的话最终两个模型的价值函数$V(\cdot)$将会收敛成只能平局的样子；2、它们会学到不同的策略，因为在训练开始时两个模型由于探索过程的随机性会产生不同的棋路，也就是说两个模型说面对的环境是不一样的，故它们会学习到不同的策略。 Translation of answer: 1. I believe that in the early stages of training, the value functions $V(\cdot)$ of both agents will fluctuate. This is because, at the beginning, the models are not yet proficient at playing Tic-Tac-Toe. As a result,&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
						new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
						j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
						'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
					})(window,document,'script','dataLayer','G-LFZQF057RK');</script><link rel="canonical" href="https://cellnw.github.io/qiang-hua-xue-xi-di-yi-zhang-xi-ti-da-an.html"><link rel="alternate" type="application/atom+xml" href="https://cellnw.github.io/feed.xml" title="cellnw&#x27;s Blog - RSS"><link rel="alternate" type="application/json" href="https://cellnw.github.io/feed.json" title="cellnw&#x27;s Blog - JSON"><meta property="og:title" content="强化学习第一章习题答案"><meta property="og:site_name" content="cellnw's Blog"><meta property="og:description" content="以下为个人答案、官方答案在文末 问： 假设上面的强化学习算法不是对战随机对手，而是以左右互博的方式自己与自己对战来训练自己。你认为在这种情况下会发生怎样的事情？它是否会学习到不同的策略？ 答： 1、我认为在训练初期，两者的价值函数$V(\cdot)$会震荡，这是因为在训练初期，模型并不是很会玩井字棋，所以两个模型一方面学会利用对方的破绽，另一方面也会弥补自己的破绽，所以价值函数$V(\cdot)$会随着这一过程变化。如果训练顺利的话最终两个模型的价值函数$V(\cdot)$将会收敛成只能平局的样子；2、它们会学到不同的策略，因为在训练开始时两个模型由于探索过程的随机性会产生不同的棋路，也就是说两个模型说面对的环境是不一样的，故它们会学习到不同的策略。 Translation of answer: 1. I believe that in the early stages of training, the value functions $V(\cdot)$ of both agents will fluctuate. This is because, at the beginning, the models are not yet proficient at playing Tic-Tac-Toe. As a result,&hellip;"><meta property="og:url" content="https://cellnw.github.io/qiang-hua-xue-xi-di-yi-zhang-xi-ti-da-an.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://cellnw.github.io/assets/css/style.css?v=11bee19e86e8eda2cf9a60efd975666d"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://cellnw.github.io/qiang-hua-xue-xi-di-yi-zhang-xi-ti-da-an.html"},"headline":"强化学习第一章习题答案","datePublished":"2025-06-13T08:53+08:00","dateModified":"2025-06-17T08:40+08:00","description":"以下为个人答案、官方答案在文末 问： 假设上面的强化学习算法不是对战随机对手，而是以左右互博的方式自己与自己对战来训练自己。你认为在这种情况下会发生怎样的事情？它是否会学习到不同的策略？ 答： 1、我认为在训练初期，两者的价值函数$V(\\cdot)$会震荡，这是因为在训练初期，模型并不是很会玩井字棋，所以两个模型一方面学会利用对方的破绽，另一方面也会弥补自己的破绽，所以价值函数$V(\\cdot)$会随着这一过程变化。如果训练顺利的话最终两个模型的价值函数$V(\\cdot)$将会收敛成只能平局的样子；2、它们会学到不同的策略，因为在训练开始时两个模型由于探索过程的随机性会产生不同的棋路，也就是说两个模型说面对的环境是不一样的，故它们会学习到不同的策略。 Translation of answer: 1. I believe that in the early stages of training, the value functions $V(\\cdot)$ of both agents will fluctuate. This is because, at the beginning, the models are not yet proficient at playing Tic-Tac-Toe. As a result,&hellip;","author":{"@type":"Person","name":"cellnw","url":"https://cellnw.github.io/authors/cellnw/"},"publisher":{"@type":"Organization","name":"cellnw"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=G-LFZQF057RK" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header class="top js-header"><a class="logo" href="https://cellnw.github.io/">cellnw&#x27;s Blog</a></header><main class="post"><article class="content"><div class="hero hero--noimage"><header class="hero__content"><div class="wrapper"><h1>强化学习第一章习题答案</h1><div class="feed__meta content__meta"><a href="https://cellnw.github.io/authors/cellnw/" class="feed__author">cellnw</a> <time datetime="2025-06-13T08:53" class="feed__date">六月 13, 2025</time></div></div></header></div><div class="entry-wrapper content__entry"><blockquote><p>以下为个人答案、官方答案在文末</p></blockquote><h6 id="练习11-左右互博">练习1.1 左右互博</h6><p><strong>问：</strong> 假设上面的强化学习算法不是对战随机对手，而是以左右互博的方式自己与自己对战来训练自己。你认为在这种情况下会发生怎样的事情？它是否会学习到不同的策略？</p><p><strong>答：</strong> 1、我认为在训练初期，两者的价值函数$V(\cdot)$会震荡，这是因为在训练初期，模型并不是很会玩井字棋，所以两个模型一方面学会利用对方的破绽，另一方面也会弥补自己的破绽，所以价值函数$V(\cdot)$会随着这一过程变化。如果训练顺利的话最终两个模型的价值函数$V(\cdot)$将会收敛成只能平局的样子；2、它们会学到不同的策略，因为在训练开始时两个模型由于探索过程的随机性会产生不同的棋路，也就是说两个模型说面对的环境是不一样的，故它们会学习到不同的策略。</p><p><strong>Translation of answer:</strong> 1. I believe that in the early stages of training, the value functions $V(\cdot)$ of both agents will fluctuate. This is because, at the beginning, the models are not yet proficient at playing Tic-Tac-Toe. As a result, each model will learn to exploit the weaknesses of the other while also correcting its own flaws, causing the value functions $V(\cdot)$ to change during this process. If the training proceeds well, the value functions $V(\cdot)$ of both models will eventually converge to a state where the game always ends in a draw; 2. They will learn different strategies, because at the start of training, due to the randomness inherent in exploration, the two models will follow different paths of play. In other words, they face slightly different environments, leading them to learn distinct strategies.</p><h6 id="练习12-对称性">练习1.2 对称性</h6><p><strong>问：</strong> 由于对称性，井字棋的很多位置看起来不同但其实是相同的。我们如何利用这一点来修改上面提到的学习过程呢？这种改变会怎样改善学习过程？假设对方没有利用对称性，那我们应该利用吗？对称相等的位置是否必然具有相同的价值呢？</p><p><strong>答：</strong> 1、井字棋的棋盘具有旋转45°、90°或135°后以及水平、垂直或中心对称后棋面等价的性质，那么我们可以利用这些特性将棋面的状态进行压缩，具体来说就是将任意的棋面利用上述特性进行映射，映射成总是从左上角开始下的样子；2、这样的话价值函数$V(\cdot)$的取值范围将会变小，这样加速函数的收敛；3、我们不应当利用对称性，虽然棋面看起来是等价的，但对方没有利用对称性，那么棋面所对应后续落子的价值是不同的；4、并不是，例如上面提到的当对手没有利用对称性时，那么对称相等的位置的价值就是不同的。</p><p><strong>Translation of answer:</strong> 1. The tic-tac-toe board has symmetrical properties such as equivalence under 45°, 90°, or 135° rotations, as well as horizontal, vertical, and central reflections. We can exploit these properties to compress the state space by mapping any board configuration to a canonical form—for example, always representing the board as if the moves started from the top-left corner; 2. This compression reduces the range of the value function $V(\cdot)$, which in turn can accelerate the convergence of the learning process; 3. we should not make use of symmetry if the opponent does not exploit it. Even though board states may appear symmetrical, the value of future moves may differ depending on the opponent’s asymmetric strategy; 4. symmetric positions do not necessarily have the same value. As mentioned above, if the opponent does not account for symmetry, then symmetric positions can indeed have different expected values.</p><h6 id="练习13-贪心策略">练习1.3 贪心策略</h6><p><strong>问：</strong> 假设强化学习的玩家是贪心的，也就是说，他总是把棋子移动到他认为最好的位置，而从不进行试探。相比一个非贪心玩家，他会玩的更好，还是更差呢？可能会出现什么问题？</p><p><strong>答：</strong> 1、玩得更差；2、收敛速度会很慢并且很可能难以找到最优解。考虑一种极端的情况，玩家基于贪心的下法第一局下成了平局，而价值函数$V(\cdot)$正是用平局的价值初始化的，这代表这局游戏结束后价值函数并不会更新。那么这名玩家将会永远重复和第一局一模一样的棋局。</p><p><strong>Translation of answer:</strong> The greedy player is likely to perform worse and may converge very slowly, making it difficult to find the optimal solution; 2. Consider an extreme case: if the greedy player plays the first game and ends in a draw, and the value function $V(\cdot)$ is initialized with the value of a draw, then there will be no update to the value function after the game. As a result, the player will repeat the exact same game indefinitely, failing to explore potentially better moves.</p><h6 id="练习14-从试探中学习">练习1.4 从试探中学习</h6><p><strong>问：</strong> 假设学习更新发生在包括试探动作在内的所有动作之后，如果步长参数随着时间而适当减少（试探的趋势并不减弱），那么状态的价值将收敛到一组概率。我们从试探的行动中学习，或者不从中学习，计算两组概率（从概念上说），分别会是什么？假设我们继续进行试探性的行动，哪一组概率对于学习来说可能更好？哪一组更可能带来更大的胜率？</p><p><strong>答：</strong> 1、从概率上来说如果不从试探中学习那么最终的概率就是以贪心的下法获胜的概率，如果从试探中学习那么最终的概率就是包含了一定概率的试探的情况下的获胜概率；2、不从试探中学习对学习来说会更好，因为从试探中学习会让状态的价值具有波动，收敛慢；3、从胜率上来说从试探中学习能够带来更高的胜率，因为从试探中学习出来的概率是包含了一些扰动的。在具有扰动的概率中选择出来的下法，更加具有鲁棒性。</p><p><strong>Translation of answer:</strong> 1. If we do not learn from exploratory actions, the final probabilities conceptually represent the win rate when always acting greedily; if we do learn from exploratory actions, the final probabilities reflect the win rate under a policy that includes exploration with a certain probability; 2. From a learning perspective, not learning from exploration is generally better because incorporating exploratory outcomes into learning can introduce fluctuations in the estimated value of states, leading to slower convergence; 3. However, in terms of win rate, learning from exploratory actions may yield higher performance, since the resulting probabilities account for stochastic disturbances. Policies derived from such probabilities tend to be more robust and thus may achieve greater success in uncertain environments.</p><h6 id="练习15-其他提升方法">练习1.5 其他提升方法</h6><p><strong>问：</strong> 你能想出其他方法来提升强化学习的玩家能力吗？你能想出更好的方法来解决井字棋问题吗？</p><p><strong>答：</strong> 1、输入一些先验知识；2、例如像井字棋这样较为简单或者人类很熟练的游戏，可以根据先验知识或者人类棋手的经验给出一个初始概率，这样强化学习模型能够更快得收敛。</p><p><strong>Translation of answer:</strong> 1. One possible approach is to incorporate prior knowledge into the reinforcement learning process; 2. For example, in relatively simple games like tic-tac-toe, where human expertise is well-established, we can use prior knowledge or strategies derived from experienced human players to assign an initial probability distribution over actions. This allows the reinforcement learning model to converge more quickly and effectively by starting with a more informed policy.</p><h1 id="标准答案">标准答案</h1><p><a href="media/files/answers1.pdf">官方回复下载</a></p></div><footer class="content__footer"><div class="entry-wrapper"><p class="content__updated">This article was updated on 六月 17, 2025</p><div class="content__actions"><ul class="content__tag"><li><a href="https://cellnw.github.io/tags/reinforcement-learning/">Reinforcement Learning</a></li></ul><div class="content__share"><button class="btn--icon content__share-button js-content__share-button"><svg width="20" height="20" aria-hidden="true"><use xlink:href="https://cellnw.github.io/assets/svg/svg-map.svg#share"></use></svg> <span>Share It</span></button><div class="content__share-popup js-content__share-popup"></div></div></div><div class="content__bio bio"><div><h3 class="h4 bio__name"><a href="https://cellnw.github.io/authors/cellnw/" rel="author">cellnw</a></h3></div></div></div></footer></article><div class="content__comments"><div class="entry-wrapper"><div id="disqus_thread"></div><script>/**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://cellnw.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div><div class="banner banner--after-content"><div class="wrapper"><script>window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    svg: { fontCache: 'global' }
  };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer="defer"></script></div></div></main><footer class="footer footer--glued"><div class="wrapper"><div class="footer__copyright"><p>Powered by Publii</p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://cellnw.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://cellnw.github.io/assets/js/scripts.min.js?v=700105c316933a8202041b6415abb233"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>